{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a395afb9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üåé Air Quality Forecast App ‚Äì Los Angeles\n",
    "# Integrating WAQI (Real-time) & Meteomatics (Historical + 24hr API Forecast)\n",
    "# Using a 24hr SARIMA model for forecasting\n",
    "# ============================================\n",
    "\n",
    "# Ensure necessary libraries are installed\n",
    "# !pip install requests pandas plotly scikit-learn statsmodels\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import plotly.graph_objects as go\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings from statsmodels to keep the output clean\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# üîê User Config\n",
    "# ==============================\n",
    "CITY = \"Los Angeles\"\n",
    "LAT, LON = 34.0522, -118.2437  # Coordinates for Los Angeles\n",
    "\n",
    "# --- API TOKENS & CREDENTIALS ---\n",
    "# Get a free token from https://aqicn.org/data-platform/token/\n",
    "AQICN_API_TOKEN = \"adab0250428e88549d94621afbdaa2c1c36d00c5\"\n",
    "\n",
    "# Enter your Meteomatics API credentials here\n",
    "METEO_USER = \"menon_prithvishankar\"\n",
    "METEO_PASS = \"Q6bRXp5GWf1mLoLa7Jz1\"\n",
    "\n",
    "# ==============================\n",
    "# 1Ô∏è‚É£ Fetch Real-time Data (Air Quality Open Data Platform)\n",
    "# ==============================\n",
    "def get_waqi_data(lat, lon, token):\n",
    "    \"\"\"\n",
    "    Fetches the latest air quality data from the World Air Quality Index project.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.waqi.info/feed/geo:{lat};{lon}/?token={token}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if data.get(\"status\") == \"ok\":\n",
    "            return data[\"data\"].get(\"iaqi\", {})\n",
    "        else:\n",
    "            print(f\"‚ùå WAQI API returned an error: {data.get('data')}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Error fetching data from WAQI API: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==============================\n",
    "# 2Ô∏è‚É£ Fetch Historical & API Forecast Data (Meteomatics)\n",
    "# ==============================\n",
    "def get_air_quality_data_meteomatics(lat, lon, pollutants, username, password):\n",
    "    \"\"\"\n",
    "    Fetches historical (90 days) and forecast (24 hours) air quality data\n",
    "    from the Meteomatics API using Basic Authentication.\n",
    "    \"\"\"\n",
    "    # Mapping from WAQI names to Meteomatics parameter names\n",
    "    pollutant_mapping = {\n",
    "        'pm25': 'pm2p5:ugm3',\n",
    "        'pm10': 'pm10:ugm3',\n",
    "        'o3': 'o3:ugm3',\n",
    "        'no2': 'no2:ugm3',\n",
    "        'so2': 'so2:ugm3',\n",
    "        'co': 'co:ugm3'\n",
    "    }\n",
    "    # Create a reverse mapping to rename columns back to the simple format\n",
    "    reverse_mapping = {v: k for k, v in pollutant_mapping.items()}\n",
    "\n",
    "    # Filter for pollutants available in Meteomatics and build the parameter string\n",
    "    meteomatics_params = [pollutant_mapping[p] for p in pollutants if p in pollutant_mapping]\n",
    "    if not meteomatics_params:\n",
    "        print(\"‚ùå None of the available real-time pollutants can be mapped for Meteomatics data.\")\n",
    "        return pd.DataFrame()\n",
    "    params_str = \",\".join(meteomatics_params)\n",
    "\n",
    "    # Define the time range (90 days past to 24 hours future, hourly interval)\n",
    "    end_time = dt.datetime.utcnow() + dt.timedelta(hours=24) # <-- CHANGED\n",
    "    start_time = dt.datetime.utcnow() - dt.timedelta(days=90)\n",
    "    time_str = f\"{start_time.isoformat(timespec='seconds')}Z--{end_time.isoformat(timespec='seconds')}Z:PT1H\"\n",
    "\n",
    "    # Location string\n",
    "    location_str = f\"{lat},{lon}\"\n",
    "\n",
    "    # Construct the final API URL\n",
    "    url = f\"https://api.meteomatics.com/{time_str}/{params_str}/{location_str}/json\"\n",
    "    print(\"Querying Meteomatics API for 24-hour forecast...\")\n",
    "\n",
    "    try:\n",
    "        # Make the request with authentication\n",
    "        response = requests.get(url, auth=(username, password))\n",
    "        response.raise_for_status()  # This will raise an error for bad responses (4xx or 5xx)\n",
    "        data = response.json()\n",
    "\n",
    "        if data.get('status') != 'OK' or not data.get('data'):\n",
    "            print(f\"‚ùå Meteomatics API returned an error: {data.get('message')}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Parse the Meteomatics JSON structure into a pandas DataFrame\n",
    "        all_data = {}\n",
    "        for param_data in data['data']:\n",
    "            param_name = param_data['parameter']\n",
    "            dates = param_data['coordinates'][0]['dates']\n",
    "            values = [d['value'] for d in dates]\n",
    "            all_data[param_name] = values\n",
    "        \n",
    "        df = pd.DataFrame(all_data)\n",
    "        # The 'date' is the same for all parameters, so we can take it from the first one\n",
    "        df['datetime'] = [d['date'] for d in data['data'][0]['coordinates'][0]['dates']]\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "\n",
    "        # Rename columns to the simple format for consistency (e.g., 'pm25')\n",
    "        df.rename(columns=reverse_mapping, inplace=True)\n",
    "        \n",
    "        print(f\"‚úÖ Fetched historical & API forecast data from Meteomatics. Size: {df.shape}\")\n",
    "        return df\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"‚ùå HTTP Error fetching from Meteomatics: {e.response.status_code} {e.response.text}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An unexpected error occurred with Meteomatics API: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 3Ô∏è‚É£ Advanced Forecast (SARIMA Model)\n",
    "# ==============================\n",
    "def forecast_pollutant_sarima(df, pollutant, hours_ahead=24):\n",
    "    \"\"\"\n",
    "    Generates a forecast for a given pollutant using a SARIMA model.\n",
    "    This should be trained ONLY on historical data.\n",
    "    \"\"\"\n",
    "    df_filtered = df.dropna(subset=[pollutant]).copy()\n",
    "    if len(df_filtered) < 50:\n",
    "        print(f\"Not enough historical data to forecast {pollutant} with SARIMA.\")\n",
    "        return None\n",
    "\n",
    "    time_series = df_filtered.set_index('datetime')[pollutant].asfreq('H')\n",
    "    order = (1, 1, 1)\n",
    "    seasonal_order = (1, 1, 1, 24)\n",
    "\n",
    "    try:\n",
    "        model = ARIMA(time_series, order=order, seasonal_order=seasonal_order)\n",
    "        model_fit = model.fit()\n",
    "        forecast = model_fit.get_forecast(steps=hours_ahead)\n",
    "        forecast_df = forecast.summary_frame()\n",
    "        future_time = [time_series.index[-1] + dt.timedelta(hours=i+1) for i in range(hours_ahead)]\n",
    "\n",
    "        return pd.DataFrame({\n",
    "            \"datetime\": future_time,\n",
    "            f\"{pollutant}_forecast\": forecast_df['mean'].values,\n",
    "            f\"{pollutant}_lower_ci\": forecast_df['mean_ci_lower'].values,\n",
    "            f\"{pollutant}_upper_ci\": forecast_df['mean_ci_upper'].values\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå SARIMA model failed for {pollutant}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==============================\n",
    "# 4Ô∏è‚É£ Visualization (Plotly with All Data Series)\n",
    "# ==============================\n",
    "def plot_pollutant(historical_df, api_forecast_df, sarima_forecast_df, pollutant, city):\n",
    "    \"\"\"\n",
    "    Plots historical data, the Meteomatics API forecast, and our SARIMA forecast.\n",
    "    \"\"\"\n",
    "    pollutant_name = pollutant.upper()\n",
    "    title = f\"{pollutant_name} Data & 24-Hour Forecast Comparison ‚Äì {city}\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # 1. Historical Data (Solid Blue)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=historical_df[\"datetime\"], y=historical_df[pollutant],\n",
    "        mode='lines', name='Historical Data', line=dict(color='blue')\n",
    "    ))\n",
    "\n",
    "    # 2. API Forecast (Dashed Sky Blue)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=api_forecast_df[\"datetime\"], y=api_forecast_df[pollutant],\n",
    "        mode='lines', name='API Forecast', line=dict(color='#87CEEB', dash='dash')\n",
    "    ))\n",
    "\n",
    "    # 3. Our SARIMA Forecast (Dotted Red)\n",
    "    if sarima_forecast_df is not None:\n",
    "        forecast_col = f\"{pollutant}_forecast\"\n",
    "        lower_ci_col = f\"{pollutant}_lower_ci\"\n",
    "        upper_ci_col = f\"{pollutant}_upper_ci\"\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=sarima_forecast_df[\"datetime\"], y=sarima_forecast_df[forecast_col],\n",
    "            mode='lines', name='Our SARIMA Forecast', line=dict(color='red', dash='dot')\n",
    "        ))\n",
    "\n",
    "        # Confidence Interval for SARIMA forecast (Light Red Fill)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=pd.concat([sarima_forecast_df[\"datetime\"], sarima_forecast_df[\"datetime\"][::-1]]),\n",
    "            y=pd.concat([sarima_forecast_df[upper_ci_col], sarima_forecast_df[lower_ci_col][::-1]]),\n",
    "            fill='toself', fillcolor='rgba(255, 0, 0, 0.2)',\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            hoverinfo=\"skip\", showlegend=True, name='SARIMA 95% Confidence'\n",
    "        ))\n",
    "        \n",
    "        # Define zoom ranges\n",
    "        zoom_start = api_forecast_df[\"datetime\"].iloc[0] - dt.timedelta(hours=24) if not api_forecast_df.empty else historical_df[\"datetime\"].iloc[-1]\n",
    "        zoom_end = sarima_forecast_df[\"datetime\"].iloc[-1] + dt.timedelta(hours=1)\n",
    "        full_view_start = historical_df[\"datetime\"].min()\n",
    "        full_view_end = sarima_forecast_df[\"datetime\"].max()\n",
    "        \n",
    "    else: # Fallback if SARIMA fails\n",
    "        full_view_start = historical_df[\"datetime\"].min()\n",
    "        full_view_end = api_forecast_df[\"datetime\"].max() if not api_forecast_df.empty else historical_df[\"datetime\"].max()\n",
    "        zoom_start = api_forecast_df[\"datetime\"].iloc[0] - dt.timedelta(hours=24) if not api_forecast_df.empty else historical_df[\"datetime\"].iloc[-1]\n",
    "        zoom_end = api_forecast_df[\"datetime\"].max() if not api_forecast_df.empty else historical_df[\"datetime\"].max()\n",
    "        \n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"Time (UTC)\",\n",
    "        yaxis_title=f\"{pollutant_name} (Concentration / AQI)\",\n",
    "        legend_title_text='',\n",
    "        updatemenus=[dict(\n",
    "            type=\"buttons\", direction=\"right\",\n",
    "            x=0.57, xanchor=\"left\", y=1.15, yanchor=\"top\",\n",
    "            showactive=True,\n",
    "            buttons=list([\n",
    "                dict(label=\"Full View\", method=\"relayout\",\n",
    "                     args=[{\"xaxis.range\": [full_view_start, full_view_end]}]),\n",
    "                dict(label=\"Zoom to Forecast\", method=\"relayout\",\n",
    "                     args=[{\"xaxis.range\": [zoom_start, zoom_end]}])\n",
    "            ])\n",
    "        )]\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# ==============================\n",
    "# 5Ô∏è‚É£ Health Alerts\n",
    "# ==============================\n",
    "def generate_alerts(df, pollutant, threshold):\n",
    "    \"\"\"\n",
    "    Checks if a pollutant's forecasted value exceeds a given threshold.\n",
    "    Based on our SARIMA model's forecast.\n",
    "    \"\"\"\n",
    "    pollutant_name = pollutant.upper()\n",
    "    forecast_col = f\"{pollutant}_forecast\"\n",
    "    high_values = df[df[forecast_col] > threshold]\n",
    "    if not high_values.empty:\n",
    "        max_val = high_values[forecast_col].max()\n",
    "        print(f\"üö® ALERT! Unhealthy {pollutant_name} levels predicted by SARIMA. Max forecast: {max_val:.2f} (Threshold: {threshold})\")\n",
    "    else:\n",
    "        print(f\"‚úÖ SARIMA forecasted {pollutant_name} levels are within safe limits.\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 6Ô∏è‚É£ Main Analysis Loop\n",
    "# ==============================\n",
    "real_time_aq_data = get_waqi_data(LAT, LON, AQICN_API_TOKEN)\n",
    "if real_time_aq_data:\n",
    "    print(f\"‚úÖ Real-time data fetched for {CITY}: {list(real_time_aq_data.keys())}\")\n",
    "    available_pollutants_realtime = list(real_time_aq_data.keys())\n",
    "    \n",
    "    # Fetch combined historical and forecast data from the Meteomatics API\n",
    "    combined_df = get_air_quality_data_meteomatics(\n",
    "        LAT, LON, available_pollutants_realtime, METEO_USER, METEO_PASS\n",
    "    )\n",
    "\n",
    "    if not combined_df.empty:\n",
    "        # Define the split point (current time)\n",
    "        now_utc = pd.Timestamp.utcnow()\n",
    "        historical_df = combined_df[combined_df['datetime'] <= now_utc].copy()\n",
    "        api_forecast_df = combined_df[combined_df['datetime'] > now_utc].copy()\n",
    "        \n",
    "        print(f\"Data split: {len(historical_df)} historical points, {len(api_forecast_df)} API forecast points.\")\n",
    "\n",
    "        HEALTH_THRESHOLDS = {'pm25': 35, 'pm10': 75, 'o3': 70, 'no2': 100, 'so2': 75, 'co': 9000}\n",
    "        available_pollutants = [col for col in combined_df.columns if col not in ['datetime']]\n",
    "\n",
    "        for pollutant in available_pollutants:\n",
    "            print(f\"\\n--- Analyzing {pollutant.upper()} ---\")\n",
    "\n",
    "            # 1. Generate SARIMA Forecast using ONLY historical data\n",
    "            sarima_forecast_df = forecast_pollutant_sarima(historical_df, pollutant, hours_ahead=24) # <-- CHANGED\n",
    "\n",
    "            # 2. Plot all three data series\n",
    "            plot_pollutant(historical_df, api_forecast_df, sarima_forecast_df, pollutant, CITY)\n",
    "\n",
    "            # 3. Check for Health Alerts based on our SARIMA forecast\n",
    "            if sarima_forecast_df is not None:\n",
    "                threshold = HEALTH_THRESHOLDS.get(pollutant, 150)\n",
    "                generate_alerts(sarima_forecast_df, pollutant, threshold)\n",
    "else:\n",
    "    print(\"Could not fetch real-time data to proceed with analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749ce703",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üåé Air Quality Forecast App ‚Äì Interactive Location\n",
    "# With Fallback Logic for Guaranteed Pollutants\n",
    "# ============================================\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import plotly.graph_objects as go\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings from statsmodels to keep the output clean\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==============================\n",
    "# üîê User Config (API Credentials)\n",
    "# ==============================\n",
    "AQICN_API_TOKEN = \"adab0250428e88549d94621afbdaa2c1c36d00c5\"\n",
    "METEO_USER = \"menon_prithvishankar\"\n",
    "METEO_PASS = \"Q6bRXp5GWf1mLoLa7Jz1\"\n",
    "\n",
    "# --- Define the core pollutants we always want to find ---\n",
    "REQUIRED_POLLUTANTS = ['pm25', 'pm10', 'o3', 'no2', 'so2', 'co']\n",
    "\n",
    "# ==============================\n",
    "# ‚öôÔ∏è Helper Functions\n",
    "# ==============================\n",
    "def get_location_name(lat, lon):\n",
    "    headers = {'User-Agent': 'AirQualityForecastApp/1.0'}\n",
    "    url = f\"https://nominatim.openstreetmap.org/reverse?format=json&lat={lat}&lon={lon}\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers); response.raise_for_status(); data = response.json()\n",
    "        address = data.get('address', {}); location = address.get('city', address.get('town', address.get('village', 'Unknown Location')))\n",
    "        return location\n",
    "    except Exception: return \"Unknown Location\"\n",
    "\n",
    "def get_waqi_data(lat, lon, token):\n",
    "    url = f\"https://api.waqi.info/feed/geo:{lat};{lon}/?token={token}\"\n",
    "    try:\n",
    "        response = requests.get(url); response.raise_for_status(); data = response.json()\n",
    "        return data[\"data\"].get(\"iaqi\", {}) if data.get(\"status\") == \"ok\" else None\n",
    "    except requests.exceptions.RequestException: return None\n",
    "\n",
    "# --- NEW FALLBACK FUNCTION ---\n",
    "def get_fallback_data(city_name, missing_pollutants, token):\n",
    "    \"\"\"\n",
    "    Searches for nearby stations in the same city to find missing pollutant data.\n",
    "    \"\"\"\n",
    "    print(f\"Searching for missing pollutants in {city_name}...\")\n",
    "    found_data = {}\n",
    "    search_url = f\"https://api.waqi.info/search/?keyword={city_name}&token={token}\"\n",
    "    try:\n",
    "        res = requests.get(search_url); res.raise_for_status(); search_results = res.json()\n",
    "        if search_results.get(\"status\") != \"ok\": return {}\n",
    "\n",
    "        # Limit search to the top 5 most relevant stations\n",
    "        for station in search_results.get(\"data\", [])[:5]:\n",
    "            station_uid = station.get(\"uid\")\n",
    "            station_name = station.get(\"station\", {}).get(\"name\", \"N/A\")\n",
    "            if not missing_pollutants: break # Stop if we've found everything\n",
    "            if station_uid:\n",
    "                feed_url = f\"https://api.waqi.info/feed/@{station_uid}/?token={token}\"\n",
    "                feed_res = requests.get(feed_url); feed_res.raise_for_status(); station_data = feed_res.json()\n",
    "                if station_data.get(\"status\") == \"ok\":\n",
    "                    station_pollutants = station_data.get(\"data\", {}).get(\"iaqi\", {})\n",
    "                    for pollutant in list(missing_pollutants): # Iterate over a copy\n",
    "                        if pollutant in station_pollutants:\n",
    "                            value = station_pollutants[pollutant]\n",
    "                            value['source'] = station_name # Add source info\n",
    "                            found_data[pollutant] = value\n",
    "                            missing_pollutants.remove(pollutant)\n",
    "                            print(f\"  -> Found {pollutant.upper()} at station: {station_name}\")\n",
    "        return found_data\n",
    "    except Exception as e:\n",
    "        print(f\"  -> Fallback search failed: {e}\")\n",
    "        return {}\n",
    "\n",
    "def get_air_quality_data_meteomatics(lat, lon, pollutants, username, password):\n",
    "    pollutant_mapping = {'pm25': 'pm2p5:ugm3', 'pm10': 'pm10:ugm3', 'o3': 'o3:ugm3','no2': 'no2:ugm3', 'so2': 'so2:ugm3', 'co': 'co:ugm3'}\n",
    "    reverse_mapping = {v: k for k, v in pollutant_mapping.items()}\n",
    "    meteomatics_params = [pollutant_mapping[p] for p in pollutants if p in pollutant_mapping]\n",
    "    if not meteomatics_params: return pd.DataFrame()\n",
    "    params_str = \",\".join(meteomatics_params)\n",
    "    end_time = dt.datetime.utcnow() + dt.timedelta(hours=24)\n",
    "    start_time = dt.datetime.utcnow() - dt.timedelta(days=90)\n",
    "    time_str = f\"{start_time.isoformat(timespec='seconds')}Z--{end_time.isoformat(timespec='seconds')}Z:PT1H\"\n",
    "    location_str = f\"{lat},{lon}\"; url = f\"https://api.meteomatics.com/{time_str}/{params_str}/{location_str}/json\"\n",
    "    print(\"Querying Meteomatics API for historical & forecast data...\")\n",
    "    try:\n",
    "        response = requests.get(url, auth=(username, password)); response.raise_for_status(); data = response.json()\n",
    "        if data.get('status') != 'OK' or not data.get('data'): return pd.DataFrame()\n",
    "        all_data = {}\n",
    "        for param_data in data['data']:\n",
    "            param_name, dates = param_data['parameter'], param_data['coordinates'][0]['dates']\n",
    "            values = [d['value'] for d in dates]; all_data[param_name] = values\n",
    "        df = pd.DataFrame(all_data)\n",
    "        df['datetime'] = [d['date'] for d in data['data'][0]['coordinates'][0]['dates']]\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'], utc=True); df.rename(columns=reverse_mapping, inplace=True)\n",
    "        print(\"‚úÖ Fetched historical & API forecast data from Meteomatics.\")\n",
    "        return df\n",
    "    except Exception: return pd.DataFrame()\n",
    "\n",
    "def forecast_pollutant_sarima(df, pollutant, hours_ahead=24):\n",
    "    df_filtered = df.dropna(subset=[pollutant]).copy()\n",
    "    if len(df_filtered) < 50: print(f\"Not enough historical data to forecast {pollutant.upper()} with SARIMA.\"); return None\n",
    "    time_series = df_filtered.set_index('datetime')[pollutant].asfreq('H')\n",
    "    order, seasonal_order = (1, 1, 1), (1, 1, 1, 24)\n",
    "    try:\n",
    "        model = ARIMA(time_series, order=order, seasonal_order=seasonal_order); model_fit = model.fit()\n",
    "        forecast = model_fit.get_forecast(steps=hours_ahead); forecast_df = forecast.summary_frame()\n",
    "        future_time = [time_series.index[-1] + dt.timedelta(hours=i+1) for i in range(hours_ahead)]\n",
    "        return pd.DataFrame({\"datetime\": future_time, f\"{pollutant}_forecast\": forecast_df['mean'].values, f\"{pollutant}_lower_ci\": forecast_df['mean_ci_lower'].values, f\"{pollutant}_upper_ci\": forecast_df['mean_ci_upper'].values})\n",
    "    except Exception: return None\n",
    "\n",
    "def plot_pollutant(historical_df, api_forecast_df, sarima_forecast_df, pollutant, city):\n",
    "    pollutant_name = pollutant.upper(); title = f\"{pollutant_name} Data & 24-Hour Forecast ‚Äì {city}\"\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=historical_df[\"datetime\"], y=historical_df[pollutant], mode='lines', name='Historical Data', line=dict(color='blue')))\n",
    "    fig.add_trace(go.Scatter(x=api_forecast_df[\"datetime\"], y=api_forecast_df[pollutant], mode='lines', name='API Forecast', line=dict(color='#87CEEB', dash='dash')))\n",
    "    if sarima_forecast_df is not None:\n",
    "        forecast_col, lower_ci_col, upper_ci_col = f\"{pollutant}_forecast\", f\"{pollutant}_lower_ci\", f\"{pollutant}_upper_ci\"\n",
    "        fig.add_trace(go.Scatter(x=sarima_forecast_df[\"datetime\"], y=sarima_forecast_df[forecast_col], mode='lines', name='Our SARIMA Forecast', line=dict(color='red', dash='dot')))\n",
    "        fig.add_trace(go.Scatter(x=pd.concat([sarima_forecast_df[\"datetime\"], sarima_forecast_df[\"datetime\"][::-1]]), y=pd.concat([sarima_forecast_df[upper_ci_col], sarima_forecast_df[lower_ci_col][::-1]]), fill='toself', fillcolor='rgba(255, 0, 0, 0.2)', line=dict(color='rgba(255,255,255,0)'), hoverinfo=\"skip\", showlegend=True, name='SARIMA 95% Confidence'))\n",
    "        zoom_start = api_forecast_df[\"datetime\"].iloc[0] - dt.timedelta(hours=24) if not api_forecast_df.empty else historical_df[\"datetime\"].iloc[-1]\n",
    "        zoom_end, full_view_start, full_view_end = sarima_forecast_df[\"datetime\"].iloc[-1] + dt.timedelta(hours=1), historical_df[\"datetime\"].min(), sarima_forecast_df[\"datetime\"].max()\n",
    "    else:\n",
    "        full_view_start, full_view_end = historical_df[\"datetime\"].min(), api_forecast_df[\"datetime\"].max() if not api_forecast_df.empty else historical_df[\"datetime\"].max()\n",
    "        zoom_start, zoom_end = (api_forecast_df[\"datetime\"].iloc[0] - dt.timedelta(hours=24) if not api_forecast_df.empty else historical_df[\"datetime\"].iloc[-1]), (api_forecast_df[\"datetime\"].max() if not api_forecast_df.empty else historical_df[\"datetime\"].max())\n",
    "    fig.update_layout(title=title, xaxis_title=\"Time (UTC)\", yaxis_title=f\"{pollutant_name} (Concentration / AQI)\", updatemenus=[dict(type=\"buttons\", direction=\"right\", x=0.57, xanchor=\"left\", y=1.15, yanchor=\"top\", showactive=True, buttons=list([dict(label=\"Full View\", method=\"relayout\", args=[{\"xaxis.range\": [full_view_start, full_view_end]}]), dict(label=\"Zoom to Forecast\", method=\"relayout\", args=[{\"xaxis.range\": [zoom_start, zoom_end]}])]))])\n",
    "    fig.show()\n",
    "\n",
    "def plot_realtime_only(pollutant, data, timestamp, city):\n",
    "    pollutant_name = pollutant.upper()\n",
    "    value = data.get('v')\n",
    "    source = data.get('source') # Check for a source\n",
    "    title_source = f\"<br>(Value from nearby station: {source})\" if source else \"\"\n",
    "    title = f\"{pollutant_name} Real-Time Value ‚Äì {city}{title_source}\"\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=[timestamp], y=[value], mode='markers', marker=dict(size=15, color='red'), name='Current Value'))\n",
    "    fig.update_layout(title=title, xaxis_title=\"Time (UTC)\", yaxis_title=f\"{pollutant_name} (Concentration / AQI)\", annotations=[dict(x=timestamp, y=value, text=f\"Current: {value}\", showarrow=True, arrowhead=1, ax=20, ay=-40)])\n",
    "    fig.show()\n",
    "\n",
    "def generate_alerts(df, pollutant, threshold):\n",
    "    pollutant_name, forecast_col = pollutant.upper(), f\"{pollutant}_forecast\"\n",
    "    high_values = df[df[forecast_col] > threshold]\n",
    "    if not high_values.empty:\n",
    "        max_val = high_values[forecast_col].max()\n",
    "        print(f\"üö® ALERT! Unhealthy {pollutant_name} levels predicted. Max forecast: {max_val:.2f} (Threshold: {threshold})\")\n",
    "\n",
    "# ==============================\n",
    "# ‚ñ∂Ô∏è Main Execution Block\n",
    "# ==============================\n",
    "while True:\n",
    "    try:\n",
    "        lat_input = float(input(\"Enter the latitude (e.g., 34.0522): \")); lon_input = float(input(\"Enter the longitude (e.g., -118.2437): \"))\n",
    "        if -90 <= lat_input <= 90 and -180 <= lon_input <= 180: break\n",
    "        else: print(\"‚ùå Invalid range.\")\n",
    "    except ValueError: print(\"‚ùå Invalid input.\")\n",
    "\n",
    "city_name = get_location_name(lat_input, lon_input)\n",
    "print(f\"\\n‚ñ∂Ô∏è Running analysis for {city_name} ({lat_input}, {lon_input})...\")\n",
    "\n",
    "# --- MODIFIED DATA GATHERING LOGIC ---\n",
    "real_time_aq_data = get_waqi_data(lat_input, lon_input, AQICN_API_TOKEN)\n",
    "if real_time_aq_data is None: real_time_aq_data = {}\n",
    "\n",
    "# Check which required pollutants are missing from the initial call\n",
    "missing_pollutants = [p for p in REQUIRED_POLLUTANTS if p not in real_time_aq_data]\n",
    "\n",
    "if missing_pollutants:\n",
    "    print(f\"‚ö†Ô∏è Missing data for: {', '.join(p.upper() for p in missing_pollutants)}\")\n",
    "    fallback_data = get_fallback_data(city_name, missing_pollutants, AQICN_API_TOKEN)\n",
    "    real_time_aq_data.update(fallback_data) # Add the found data to our main dictionary\n",
    "\n",
    "# Proceed with the full list of pollutants we've gathered\n",
    "if real_time_aq_data:\n",
    "    available_pollutants = [p for p in REQUIRED_POLLUTANTS if p in real_time_aq_data]\n",
    "    print(f\"‚úÖ Using data for pollutants: {', '.join(p.upper() for p in available_pollutants)}\")\n",
    "    \n",
    "    combined_df = get_air_quality_data_meteomatics(lat_input, lon_input, available_pollutants, METEO_USER, METEO_PASS)\n",
    "    now_utc = pd.Timestamp.utcnow()\n",
    "    HEALTH_THRESHOLDS = {'pm25': 35, 'pm10': 75, 'o3': 70, 'no2': 100, 'so2': 75, 'co': 9000}\n",
    "\n",
    "    for pollutant in REQUIRED_POLLUTANTS:\n",
    "        if pollutant not in available_pollutants:\n",
    "            print(f\"\\n--- Could not find any data for {pollutant.upper()} ---\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Analyzing {pollutant.upper()} ---\")\n",
    "        if not combined_df.empty and pollutant in combined_df.columns:\n",
    "            historical_df = combined_df[combined_df['datetime'] <= now_utc].copy()\n",
    "            api_forecast_df = combined_df[combined_df['datetime'] > now_utc].copy()\n",
    "            sarima_forecast_df = forecast_pollutant_sarima(historical_df, pollutant, hours_ahead=24)\n",
    "            plot_pollutant(historical_df, api_forecast_df, sarima_forecast_df, pollutant, city_name)\n",
    "            if sarima_forecast_df is not None:\n",
    "                threshold = HEALTH_THRESHOLDS.get(pollutant, 150)\n",
    "                generate_alerts(sarima_forecast_df, pollutant, threshold)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Historical data for {pollutant.upper()} not available. Prediction is not possible.\")\n",
    "            current_data = real_time_aq_data.get(pollutant)\n",
    "            if current_data and 'v' in current_data:\n",
    "                plot_realtime_only(pollutant, current_data, now_utc, city_name)\n",
    "            else:\n",
    "                print(f\"Could not find a current value for {pollutant.upper()} to display.\")\n",
    "else:\n",
    "    print(\"Could not fetch any real-time data to begin analysis.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
